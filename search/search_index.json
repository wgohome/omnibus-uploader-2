{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Kingdom-wide Gene Expression Omnibus","text":"<p> Welcome! This is the internal documentation site for the project, managed by Mutwil Lab.</p> <p>This project aims to provide a kingdom-wide gene expression resource for all plants. The software components are broken up into several pieces and are summarised on this page. When components are found outside of this repository, links are given.</p> <p></p>"},{"location":"#frontend-application","title":"Frontend application","text":"<p>The frontend application (Plant Gene Expression Omnibus, PEO) is the interface for non-programmer biologists to interact with our resource.</p> <p>Main technologies used:</p> <ul> <li>Next.js (React.js): Frontend framework used to handle routing, rendering and create basic API endpoints. Note that functional components + hooks are used. Do not use class components anymore, it is &gt;2022. Avoid Redux, not a complexity that is needed for now.</li> <li>Tailwind CSS: CSS utility classes framework</li> <li>Mongoose: Async driver and ODM for accessing MongoDB database</li> <li>React Table (TanStack): Headless UI library for managing states for rendering tables</li> </ul> <p>Notes for future work:</p> <ul> <li>TypeScript has been loosely annotated and could be further worked on in refactoring sprints in future.</li> <li>No tests has been written for the frontend codebase yet. Perhaps a ping test for every route could be considered next as the first level of test coverage.</li> <li>Avoid adding any jquery dependency if possible.</li> <li>Go for headless libraries where possible.</li> </ul> <p>The frontend is deployed at Vercel, free-tier.</p> <p>Live application  Source code </p>"},{"location":"#database","title":"Database","text":"<p>MongoDB database is used for its flexibility in data modeling, and also to avoid expensive join operations on reads. More details can be found on the database schema page.</p> <p>Deployed at MongoDB Atlas service, hosted on AWS. We are currently migrating the Atlas instance from serverless to managed cluster to reduce costs. Refer to Marek for access to the Atlas dashboard.</p>"},{"location":"#backend-api-service","title":"Backend API service","text":"Deprecated <p>The backend API service has been deprecated in favor of directly accessing the database from the Next.js server-side application for reads, and directly writing via the Python-based uploader. This is because uploads took too long via the API and data validation and shaping could be done locally instead of on the cloud, given that only a single user (the resource admin) will be performing writes to the database.</p> <p>Main technologies used:</p> <ul> <li>Python &gt; 3.10: For the modern typehint syntax</li> <li>FastAPI (Python): Modern lightweight API framework, similar to Flask</li> <li>Pydantic: For data validation and settings management using Python type annotations</li> <li>Pytest: For writing and running tests</li> <li>Pymongo: Driver for MongoDB. Does not support asyncronous operations. Motor, the syncronous version is still problematic to use for now. An ODM - Beanie was tested but it did not make sense to use an ODM in our case.</li> </ul> <p>Source code </p>"},{"location":"#uploader","title":"Uploader","text":"<p>This docs is placed within the uploader repository.</p> <p>Source code </p>"},{"location":"#diamond-sequence-search-api-service","title":"Diamond sequence search API service","text":"<p>To enable users to identify their gene of interest through a protein sequence, a protein search tool is required. The tool should then be deplyed and provisioned to the user-facing application.</p>"},{"location":"#choice-of-sequence-search-tool-diamond","title":"Choice of sequence search tool: Diamond","text":"<p>Diamond was chosen as the protein search tool to identify genes from given protein sequences, because of its speed \ud83d\udca8, low memory footprint \ud83d\udc3e and comparable accuracy \ud83c\udfaf. Speed is essential in a user facing application, while laser accuracy makes less of a difference to user experience since the user will be choosing from the recommended search result anyway.</p>"},{"location":"#data-processing","title":"Data processing","text":"<p>Protein sequences from processed PEP files here. Note that one species (GOSAR, taxid 29729) does not have an available PEP file and is hence not searchable by protein sequence. The PEP files were used to build a diamond database, which is the reference target used when searching for an unknown protein sequence.</p>"},{"location":"#availability","title":"Availability","text":"<p>The API service is deployed on Google Cloud Run. Refer to Marek for access to the GCP project dashboard.</p> <p>Live Swagger docs  Source code </p>"},{"location":"data-processing/","title":"Overview of data","text":""},{"location":"data-processing/#species-list","title":"Species list","text":"<p>A total of 103 species are included in this iteration of PEO. The details of metadata on this species can be found at this google sheet, which was maintained by Erielle and Marek.</p> <p> </p> <p>Work in Progress</p> <p> To update with the Quality Control values used by Erielle and Marek.</p>"},{"location":"data-processing/mapman_annotations/","title":"Mapman annotations","text":""},{"location":"data-processing/mapman_annotations/#data-source","title":"Data source","text":""},{"location":"data-processing/mapman_annotations/#data-processing","title":"Data processing","text":""},{"location":"data-processing/mapman_annotations/#data-availability","title":"Data availability","text":"<p>The processed Mapman annotation files are available here.</p>"},{"location":"data-processing/po_annotations/","title":"Plant Ontology annotations","text":""},{"location":"data-processing/po_annotations/#data-source","title":"Data source","text":"<p>RNAseq samples were annotated by a team in another project led by Erielle and Marek Mutwil.</p>"},{"location":"data-processing/po_annotations/#data-processing","title":"Data processing","text":"<p>Processing and standardisation of the Plant Ontology annotations were done by Erielle.</p>"},{"location":"data-processing/po_annotations/#data-availability","title":"Data availability","text":"<p>The processed PEP files are available here.</p>"},{"location":"data-processing/processed_files/","title":"Expected format of processed input files","text":""},{"location":"data-processing/processed_files/#source-raw-files","title":"Source raw files","text":""},{"location":"data-processing/processed_files/#processed-files","title":"Processed files","text":""},{"location":"data-processing/protein_sequences/","title":"Protein sequences","text":"<p>Protein sequences are written as PEP fasta files.</p>"},{"location":"data-processing/protein_sequences/#data-source","title":"Data source","text":"<p>PEP files were obtained by Erielle, with sources documented here. # TODO: add link</p>"},{"location":"data-processing/protein_sequences/#data-processing","title":"Data processing","text":"<p>Objective</p> <p>We needed to preprocess input files involving gene entities, to ensure consistent gene identifiers used throughout the system.</p> <p>TLDR</p> <p>If you are too lazy busy to read, the processing can be summarized in this Google Sheets</p>"},{"location":"data-processing/protein_sequences/#the-problem","title":"The problem","text":"<p>We found differences in identifiers in the CDS fasta files and in the PEP fasta files.</p> <p>Mismatch problem:</p> <p></p> <p>Our data cleaning step:</p> <p></p> <p>As gene identifiers differ between the CDS/TPM file and the PEP file in unique and varied ways, two steps were taken for the cleaning.</p> <ol> <li> <p>Exploratory analysis to identify how the identifiers need to be parsed for the different species.</p> </li> <li> <p>Assigning the parsing rules to each species files and generating new pep files with the identifiers modified (if neccessary) to match the identifiers in the CDS/TPM file.</p> </li> </ol>"},{"location":"data-processing/protein_sequences/#exploratory-analysis","title":"Exploratory analysis","text":"<p>Gene identifiers were extracted from both the TPM matrices and the PEP files. As the PEP files are in fasta format, the gene identifier was obtained from the first word after each <code>&gt;</code>. The identifiers are expected to be standardised to uppercase.</p> <p>An initial quick matching of gene identifiers was performed for each species. While this may not be the most precise comparison, it helps avoid <code>n x n</code> comparisons, while providing sufficient initial exploratory metrics. This narrows down the species which require more refined matching methods. This rough exploratory step is justified as there could be undefined variations in the ways gene identifiers differ for different species.</p> <p>In this exploratory step, the identifiers from TPM and PEP sources were alphabetically sorted respectively. The array of identifiers from TPM was iterated as the point of reference, and comparison for each species was performed as such:</p> Logic for rough array comparison<pre><code>Iterate identifiers in TPM array\n    Check if the current identifier in PEP array fulfill any of:\n        - matches (`identical`)\n        - TPM identifier is a subset of PEP identifier (`pep_longer`)\n        - PEP identifier is a subset of TPM identifier (`pep_shorter`)\n    If none, then this TPM identifier is deemed to have no match\n        then move to the next identifier in PEP array\n</code></pre> <p>Implication</p> <p>This means that if there is a TPM identifier that cannot be matched to any PEP identifier, then the matching for the remaning TPM identifier will not be performed anymore.</p> <p>This is planned for as we first want to filter out species that are easy to match. The remaning species falling under this trap can be further analysed in subsequent step as they would involve more computationally intensive comparisons.</p>"},{"location":"data-processing/protein_sequences/#initial-exploratory-stats","title":"Initial exploratory stats","text":"<p>The exploratory statistics are summarised in this table, which can also be viewed in Google Sheets.</p> <p> </p> <p>Columns</p> <ul> <li>tpm_length: number of genes in TPM file</li> <li>pep_length: number of genes in PEP file</li> <li>identical: number of gene identifiers that are idnetical</li> <li>pep_longer: number of gene identifiers in TPM file which are a substring of that in PEP file</li> <li>pep_shorter: number of gene identifiers in PEP file which are a substring of that in TPM file</li> <li>pep_not_found: number of genes in TPM file, for which no matching identifier found in PEP file</li> </ul> Row 1 - 44 <p>The rows has been manually rearranged. The top rows, where \"HOW TO FIX\" column has no value, it implies perfect matching of gene identifiers and they just need to be captialised in the PEP files.</p> Row 45 - 59 <p>These species have fewer genes in the PEP file and all the genes in the PEP file have been mapped to an indentifier in the TPM matrices. So the TPM indentifiers can safely be known to be true \"no match\".</p> <p>Remaining rows</p> <p>The remaining rows were analysed individually. The examples and fixes are detailed in the Google Sheet too.</p>"},{"location":"data-processing/protein_sequences/#heuristic-for-replacing-identifiers","title":"Heuristic for replacing identifiers","text":"<p>These heuristics were used for preprocessing the gene identifiers.</p> <ul> <li> <p>Gene identifiers are capitalised to standardise across our use-case.</p> </li> <li> <p>Check if identifiers in TPM matrix/CDS matches or is a subset of PEP's. If yes, then the identifier in PEP file is replaced.</p> </li> <li> <p>If TPM's identifier don't match any in PEP, then</p> <ul> <li>The gene shall have no matching gene annotation for PFAM</li> <li>The gene will not be searchable by protein sequence</li> </ul> </li> <li> <p>If PEP's identifier don't match any in TPM, then</p> <ul> <li>The gene entry can be removed/ignored from the PEP file</li> </ul> </li> <li> <p>Sometimes, the PEP files comprises multiple isoforms. The symptoms of such cases would be when there are more gene identifiers in PEP file than in TPM file. (For most taxids, the number is higher for TPM file) To deal with this, we have to select the matching isoform, or if there is no perfect match in labels, then we can select any one isoform.</p> </li> <li> <p>In some cases the replacement gene identifiers are found within the header line of sequences. We have to manually parse this out for some species.</p> </li> </ul>"},{"location":"data-processing/protein_sequences/#fasta-relabeller-source-code","title":"Fasta relabeller source code","text":"<p>Utility class to relabel fasta sequence identifiers.</p>"},{"location":"data-processing/protein_sequences/#parsers-assigned-for-each-species","title":"Parsers assigned for each species","text":"<p>Refer to the <code>jobs</code> array to see what parsers are exactly assigned to each species.</p>"},{"location":"data-processing/protein_sequences/#validation-stats","title":"Validation stats","text":"<p>We show that most of our PEP gene identifiers were mapped to a gene indentifier in TPM file. Looking at the % matched column shows a good matching of gene identifiers.</p> <p> </p>"},{"location":"data-processing/protein_sequences/#data-availability","title":"Data availability","text":"<p>The cleaned pep files are available here.</p>"},{"location":"data-processing/tpm_matrices/","title":"Gene expression matrices","text":"<p>Gene expression is quantified in transcipts per million (TPM) by kallisto, and is presented as a two-dimensional matrix of gene against RNA-seq samples for each species.</p>"},{"location":"data-processing/tpm_matrices/#data-source","title":"Data source","text":"<p>Data is obtained from previous studies. Species and data list curated by Marek Mutwil and Erielle Villanueva. More details on the download and processing pipeline used can be found in the data sources listed below.</p> <ul> <li>LSTrAP-Kingdom</li> <li>EVOREPRO</li> <li>Manually processed by Marek Mutwil (Sesanum indicum, Lactuca sativa, Triticum dicoccoides), using LSTrAP-Cloud</li> </ul> <p>A record CDS source and the source project from which matrices are derived from are documented in this Google sheet: # TODO: embed gooogle sheet</p>"},{"location":"data-processing/tpm_matrices/#data-processing","title":"Data processing","text":"<p>Quality control on the samples of the expression matrices have already been performed by the each source projects. Gene identifiers used in the expression matrices are used as the point of reference throughout this project.</p>"},{"location":"data-processing/tpm_matrices/#data-availability","title":"Data availability","text":"<p>The processed PEP files are available here.</p>"},{"location":"db/db_hosting/","title":"MongoDB hosting","text":""},{"location":"db/schema/","title":"Database schema","text":""},{"location":"uploading/","title":"Design of the uploader","text":"<p>The uploader is designed as a set of object-oriented utilities that is modular and therefore hopefully decoupled and more easily configurable. The main parts of the uploader includes:</p> Component Description Models Model classes that inherits from Pydantic's base class, to define the data shape and validations Parsers Parser classes that parses files and shape them via models Controllers Controller classes wraps uploading logic for certain parts of the data, glueing together the parsers, models and utility functions to upload to the database. <p>Utility functions</p> Directory Description /uploader/db_setup/ Functions to setup DB, get DB driver instance, get DB collection, setup DB indexes (if does not exists yet), based on the DATABASE_URL AND DATABASE_NAME defined in the <code>.env</code> file. /uploader/db_queries/ Functions to read or write to the DB <p>Integration tests are written and run using pytest. Tests are all in the <code>/tests</code> directory and can be run with pytest.</p> <p>Data filepaths are defined in <code>config/filepath_definitions.py</code> and can be configured.</p>"},{"location":"uploading/#models","title":"Models","text":"<p>Work in Progress</p> <p> To be updated</p>"},{"location":"uploading/#parsers","title":"Parsers","text":"<p>Work in Progress</p> <p> To be updated</p>"},{"location":"uploading/#controllers","title":"Controllers","text":"<p>Work in Progress</p> <p> To be updated</p>"},{"location":"uploading/files_preparation/","title":"Files to prepare","text":"<p>The sources of data files are described in the data section.</p> <p>The default file parsers are designed in a way that expects certain files in a certain format, though you are free to extend the parsers to parse in whatever way that is suitable in future. This page describes the format for input files that is expected by the default parsers.</p>"},{"location":"uploading/setup/","title":"Setting up and managing the repository","text":""},{"location":"uploading/setup/#for-the-first-time","title":"For the first time","text":"<p>Clone the repository to your local machine.</p> <pre><code>git clone git@github.com:wirriamm/omnibus-uploader-2.git\n</code></pre> <p>Setup the local Python virtual environment, using Python &gt;=3.10 . You may use other virutal environment other than venv.</p> <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre> <p>Install Python dependencies.</p> <pre><code>pip install --upgrade pip\npip install -r requirements.txt\n</code></pre> <p>For subsequent runs, these steps do not need to be repeated anymore.</p>"},{"location":"uploading/setup/#configurations","title":"Configurations","text":"<p>Create the <code>.env</code> file to store environment variables. This file is not checked into version control as it may contain secrets. However, a template <code>.env.example</code> is provided and checked into version control, which can be used to create the <code>.env</code> file.</p> <pre><code>cp .env.example .env\n</code></pre> <p>Then open <code>.env</code> file and update the variables.</p> <ul> <li>DATA_DIR: This is the directory in which the processed files are stored and parsed for uploading.</li> <li>DATABASE_URL: The database connection url. \"mongodb://localhost:27017\" for local database instance. For production, go to Atlas to retrieve the URL.</li> <li>DATABASE_NAME: A unique string to name the database, in a single word without whitespace</li> </ul>"},{"location":"uploading/setup/#for-each-run","title":"For each run","text":"<p>Activate the virutal environment. This step has to be done everytime you begin work with the repository in a new shell.</p> <pre><code>source venv/bin/activate\n</code></pre> <p>You are now ready to run the code scripts.</p>"},{"location":"uploading/setup/#for-contributing-changes-to-the-repository","title":"For contributing changes to the repository","text":"<p>Before making any changes, checkout a new branch. This section is not needed if you just need to run the scripts and not store any changes to the code to the remote repository on Github.</p> <pre><code>git co -b feat/&lt;new-feature-name&gt;\n</code></pre> <p>Then, make your changes in the new branch and commit your changes to the source code.</p> <pre><code>git add .\ngit commit -m \"feat: &lt;describe your feature&gt;\"\ngit status\n</code></pre> <p>Once you are done commiting your changes, push to the remote repository on Github</p> <pre><code>git push origin feat/&lt;new-feature-name&gt;\n</code></pre> <p>Then, go to Github https://github.com/wirriamm/omnibus-uploader-2/pulls. Open a new pull request (PR) for merging your changes to the main branch. Describe the changes in the PR. Wait for code owner to approve your changes and merge it to the main branch.</p> <p>Once the PR has been merged to main branch, you have to update your local main branch by pulling from the remote (Github) main branch.</p> <pre><code>git co main\ngit pull origin main\n</code></pre>"},{"location":"uploading/uploading_steps/","title":"Uploading data","text":""},{"location":"uploading/uploading_steps/#check-configurations","title":"Check configurations","text":"<p>Before uploading, check if your database and data source configurations (described here) are as desired.</p> <ul> <li>DATABASE_URL</li> <li>DATABASE_NAME</li> <li>DATA_DIR</li> </ul> <p>Also ensure that the python environment has been activated, as described here.</p>"},{"location":"uploading/uploading_steps/#default-upload-script","title":"Default upload script","text":"<p>The standard uploader script is found at <code>uploader/scripts/main.py</code>. Run this script to run the upload.</p> <pre><code>cd ==&lt;root directory of this repository&gt;==\nexport PYTHONPATH=.\npython uploader/scripts/main.py\n</code></pre> <p>If you would like to run it as a background process:</p> <pre><code>nohup python uploader/scripts/main.py &gt; upload.out &amp;\n</code></pre> <p>If you would like to have a customised solution, create another python file in the <code>/uploader/scripts/</code> dir and you can run it on its own. Alternatively, modify the <code>/uploader/scripts/main.py</code> file to suit your needs.</p>"}]}